# -*- coding: utf-8 -*-

"""k-means

Automatically generated by Colab.

Original file is located at
[https://colab.research.google.com/drive/1xRR8lbVFK2AoktfopDfVW_4aM8C9KL6W](https://colab.research.google.com/drive/1xRR8lbVFK2AoktfopDfVW_4aM8C9KL6W)
"""

import subprocess
import sys

# Install necessary packages
subprocess.check_call([sys.executable, "-m", "pip", "install", "scikit-plot"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "pycaret"])

# Importing necessary libraries
import pandas as pd  # For data manipulation using DataFrames
import seaborn as sns  # For statistical data visualization
import matplotlib.pyplot as plt  # For creating static, interactive, and animated visualizations in Python
import numpy as np  # For numerical operations
from numpy import inf  # Importing 'inf' (infinity) from numpy
import scipy  # For scientific computing
from scipy.stats import yeojohnson  # For Yeojohnson transformation
import scipy.stats as stats  # Statistical functions
from scipy.stats import jarque_bera  # Jarque-Bera test for normality
from scipy.stats import normaltest  # D'Agostino and Pearson's normality test
from scipy.stats import shapiro  # Shapiro-Wilk test for normality
import sklearn  # For machine learning tools
from sklearn import linear_model  # Linear regression model
from sklearn.preprocessing import StandardScaler  # Standardize features by removing the mean and scaling to unit variance
from sklearn.preprocessing import MinMaxScaler  # Transform features by scaling each feature to a given range
from sklearn.model_selection import train_test_split, cross_val_score  # Train-test split and cross-validation
from sklearn import metrics  # Metrics for evaluating model performance
from sklearn.metrics import mean_absolute_error  # Mean absolute error metric
from sklearn import preprocessing  # Data preprocessing
from sklearn.decomposition import PCA  # Principal Component Analysis
from sklearn.cluster import DBSCAN  # Density-based spatial clustering of applications with noise
from sklearn.metrics import silhouette_score  # Silhouette score for clustering evaluation
from sklearn.cluster import KMeans  # KMeans clustering algorithm
import plotly.express as px  # Interactive visualization library
import plotly.graph_objects as go  # Create and modify plots using a high-level interface
import scikitplot as skplt  # Visualization for machine learning
import statsmodels.api as sm  # Statistical models library
from statsmodels.tools.eval_measures import mse, rmse  # Mean squared error and root mean squared error
from statsmodels.tsa.stattools import acf  # Autocorrelation function for time series analysis
import tensorflow as tf  # Open-source machine learning framework
from yellowbrick.cluster import KElbowVisualizer  # Visualizer for finding the optimal number of clusters
import warnings  # Ignore warning messages during execution

# Ignore warning messages in the output
warnings.filterwarnings('ignore')

# Read Spotify data from CSV file using Pandas
df = pd.read_csv(r'spotify.csv')

"""### Memeriksa data"""
df.head()

# Display information about the DataFrame, such as data types and number of non-null values
df.info()

# Count the number of null (NaN) values in each column of the DataFrame
df.isnull().sum()

# Identify and extract rows that are duplicates in the DataFrame
duplicates = df[df.duplicated()]

# Display duplicate rows
print("Baris Duplikat kecuali kemunculan pertama berdasarkan semua kolom:")
print(duplicates)

# Display the number of rows and columns in the DataFrame
print("Shape of DataFrame:", df.shape)

# Display summary statistics for numeric columns in the DataFrame
print(df.describe())

"""### Data Cleaning"""

# Drop irrelevant categories
df = df.drop(['track_id', 'track_artist', 'track_album_name', 'track_name', 'track_album_id', 'playlist_id', 'track_album_release_date', 'playlist_subgenre'], axis=1)

# Display the first five rows in the DataFrame
df.head()

# Display the number of rows and columns in the DataFrame
print("Shape of DataFrame after dropping columns:", df.shape)

# Filter the DataFrame based on 'speechiness' values less than or equal to 0.7
df = df[df['speechiness'] <= 0.7]

# Display information about the DataFrame after filtering
df.info()

# Identify numeric and categorical columns in the DataFrame
numeric_cols = []  # List to hold numeric column names
categorical_cols = []  # List to hold categorical column names

# Iterate through each column in the DataFrame
for col in df.columns:
    # Check the data type of the column; if float64 or int64, consider it numeric
    if df[col].dtype == np.float64 or df[col].dtype == np.int64:
        numeric_cols.append(col)  # Add numeric column name to numeric_cols list
    else:
        categorical_cols.append(col)  # Add categorical column name to categorical_cols list

# Display the identified numeric and categorical columns
print('Kolom numerik:', numeric_cols)
print('Kolom kategorikal:', categorical_cols)

# Separate discrete numeric columns
discrete_numeric = [feature for feature in numeric_cols if df[feature].nunique() < 20]

# Display the list of discrete numeric columns
print("Discrete numeric columns:", discrete_numeric)

# Replace values in 'mode' column: 0 with 'minor', 1 with 'major'
df['mode'] = df['mode'].replace({0: 'minor', 1: 'major'})

# Replace values in 'key' column with musical key names
df['key'] = df['key'].replace({
    0: 'C', 1: 'C-sharp_D-flat', 2: 'D', 3: 'D-sharp_E-flat',
    4: 'E', 5: 'F', 6: 'F-sharp_G-flat', 7: 'G',
    8: 'G-sharp_A-flat', 9: 'A', 10: 'A-sharp_B-flat', 11: 'B'
})

# Remove all rows where "duration_ms" is equal to 0
df = df[df['duration_ms'] != 0]

# Display the first five rows in the DataFrame
df.head()

# Re-identify numeric and categorical columns after data transformation
numeric_cols = []  # List to hold numeric column names
categorical_cols = []  # List to hold categorical column names

# Iterate through each column in the modified DataFrame
for col in df.columns:
    # Check the data type of the column; if float64 or int64, consider it numeric
    if df[col].dtype == np.float64 or df[col].dtype == np.int64:
        numeric_cols.append(col)  # Add numeric column name to numeric_cols list
    else:
        categorical_cols.append(col)  # Add categorical column name to categorical_cols list

# Display the identified numeric and categorical columns after data transformation
print('Kolom numerik:', numeric_cols)
print('Kolom kategorikal:', categorical_cols)

#### Checking for Outliers

# Check for the presence of outliers
max_rows = 50  # Limit the number of rows to display

# Calculate Z-Score for each value in numeric columns
z_scores = (df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std()

# Find absolute Z-Scores greater than threshold 3
threshold = 3
outliers = (z_scores.abs() > threshold).any(axis=1)

# Get the layout (style) of outliers and limit the number of rows displayed
outliers_style = df[outliers].style.background_gradient(cmap='Reds').set_table_attributes("style='display: table-row-group;'").set_caption(f"Outliers ({outliers.sum()} baris)")
outliers_html = outliers_style.render()

# Get the header of the table and limit the number of rows displayed
header = df.head(0).style.set_table_attributes("style='display: none;'").render()
data = df.head(max_rows if len(df) > max_rows else len(df)).style.render()

# Create box plot for each column in numeric data
# Set up subplot with one row and number of columns equal to the number of numeric columns
fig, axes = plt.subplots(nrows=1, ncols=len(numeric_cols), figsize=(28, 5))

# Create box plot for each numeric column
for i, col in enumerate(numeric_cols):
    axes[i].boxplot(df[col], vert=False)  # Create box plot with horizontal orientation
    # Set title, x-label, and y-label for each subplot
    axes[i].set_title(f"Boxplot of {col}")
    axes[i].set_xlabel("Feature Value")
    axes[i].set_ylabel(col)

# Adjust layout of subplots for better appearance
fig.tight_layout()

# Display the plot
plt.show()

# Calculate the percentage of outliers for each numeric feature using the IQR method
for col in numeric_cols:
    q1 = df[col].quantile(0.25)  # Calculate first quartile
    q3 = df[col].quantile(0.75)  # Calculate third quartile
    iqr = q3 - q1  # Calculate Interquartile Range (IQR)
    upper_boundary = q3 + 1.5 * iqr  # Upper boundary for identifying outliers
    lower_boundary = q1 - 1.5 * iqr  # Lower boundary for identifying outliers
    outliers = df[(df[col] < lower_boundary) | (df[col] > upper_boundary)][col]  # Identify outliers
    outlier_percent = outliers.count() / df[col].count() * 100  # Calculate percentage of outliers
    print(f"{col} memiliki {outlier_percent:.2f}% outliers")

# Yeo-Johnson transformation for certain features
transformed_data = df.copy()

# Perform Yeojohnson transformation on 'duration_ms', 'loudness', and 'liveness'
transformed_data['duration_ms'], duration_lambda = yeojohnson(transformed_data['duration_ms'])
transformed_data['loudness'], loudness_lambda = yeojohnson(transformed_data['loudness'])
transformed_data['liveness'], liveness_lambda = yeojohnson(transformed_data['liveness'])

# Display summary statistics after transformation
print(transformed_data[['duration_ms', 'loudness', 'liveness']].describe())

# Calculate the percentage of outliers after Yeojohnson transformation for some numeric features
num_cols = ['track_popularity', 'duration_ms', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']
df_num = transformed_data[num_cols]

for col in num_cols:
    q1 = transformed_data[col].quantile(0.25)  # Calculate first quartile
    q3 = transformed_data[col].quantile(0.75)  # Calculate third quartile
    iqr = q3 - q1  # Calculate Interquartile Range (IQR)
    upper_boundary = q3 + 1.5 * iqr  # Upper boundary for identifying outliers
    lower_boundary = q1 - 1.5 * iqr  # Lower boundary for identifying outliers
    outliers = transformed_data[(transformed_data[col] < lower_boundary) | (transformed_data[col] > upper_boundary)][col]  # Identify outliers
    outlier_percent = outliers.count() / transformed_data[col].count() * 100  # Calculate percentage of outliers
    print(f"{col} memiliki {outlier_percent:.2f}% outliers")

# Checking Numerical Features Distribution
df_t = transformed_data.copy()

# Display distribution plots for numeric features after transformation
n_cols = len(num_cols)
n_rows = int(np.ceil(n_cols / 3))

# Create subplot with number of rows and columns according to the number of numeric features
fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5 * n_rows))

# Iterate through each numeric feature and create distribution plots using Seaborn
for i, col in enumerate(num_cols):
    row_idx = i // 3
    col_idx = i % 3
    sns.histplot(df_t[col], ax=axes[row_idx, col_idx], kde=True)

    # Set title and x-label for each subplot
    axes[row_idx, col_idx].set_title(f"Distribution Plot of {col}")
    axes[row_idx, col_idx].set_xlabel(col)

# Remove unused subplots if the number of features is not divisible by 3
for i in range(n_cols, n_rows * 3):
    fig.delaxes(axes.flatten()[i])

# Adjust layout of subplots for better appearance
fig.tight_layout()

# Display the plot
plt.show()

# Perform Kolmogorov-Smirnov test to check normal distribution on each numeric feature
data = df_t[['track_popularity', 'duration_ms', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']]

# Iterate through each column and perform K-S test
for col in data.columns:
    kstest_result = stats.kstest(data[col], 'norm')

    # Display test statistic and p-value for each column
    print(f"{col} - K-S test statistic:", kstest_result.statistic)
    print(f"{col} - p-value:", kstest_result.pvalue)

# Check if transformation can yield a more normal distribution
num_cols = ['track_popularity', 'duration_ms', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']

new_df = pd.DataFrame()

# Loop through each column and possible transformations
for col_name in num_cols:
    new_df[f'log({col_name})'] = np.log1p(df[col_name])  # Log transformation
    new_df[f'sqrt({col_name})'] = np.sqrt(df[col_name])   # Square root transformation
    new_df[f'exp({col_name})'] = np.exp(df[col_name])     # Exponential transformation

plot_cols = 3

fig_width = 8
fig_height = 25
subplot_width = fig_width / plot_cols
subplot_height = fig_height / (len(num_cols) / plot_cols)

# Create subplots for each column and transformation with Seaborn
fig, axes = plt.subplots(len(num_cols), plot_cols, figsize=(fig_width, fig_height))

for i, col_name in enumerate(num_cols):
    for j, transform in enumerate(['log', 'sqrt', 'exp']):
        plot_data = new_df[f"{transform}({col_name})"]
        plot_title = f"{transform}({col_name})"
        ax = axes[i, j]
        sns.kdeplot(plot_data, ax=ax)
        ax.set_title(plot_title)

# Adjust spacing and labels
plt.tight_layout()
plt.subplots_adjust(hspace=0.6, wspace=0.3)
plt.show()

# No transformation visually improved any feature significantly. Therefore, log, square, or exponential transformations will not be applied.

#### Data Exploration and Feature Engineering

# Display the first few rows of DataFrame df_t
df_t.head()

# Calculate correlation between numeric features and display as heatmap
num_cols = df_t.select_dtypes(include=['int64', 'float64'])

# Calculate correlation matrix between numeric features
corr_data = num_cols.corr()

# Create subplot heatmap
fig, ax = plt.subplots(figsize=(10, 10))

# Display heatmap with annotations (correlation values shown in heatmap cells)
sns.heatmap(corr_data, cmap="YlGnBu", annot=True)

# Display the plot
plt.show()

# No continuous features correlate with the target (track_popularity)
# Loudness and energy show multicollinearity (0.67). Since "energy" is a subjective rating versus "loudness" which is measured, energy will be removed from the dataset.
df_t = df_t.drop(['energy'], axis=1)

#### Checking the Balance of Categorical Features

# Identify continuous and categorical columns in DataFrame df_t
continuous_cols = []
categorical_cols = []

# Iterate through each column in DataFrame df_t
for col in df_t.columns:
    # Check the data type of the column (float64 or int64)
    if df_t[col].dtype == np.float64 or df_t[col].dtype == np.int64:
        continuous_cols.append(col)  # If data type is numeric, add to continuous columns
    else:
        categorical_cols.append(col)  # If data type is not numeric, add to categorical columns

# Display the results
print('Continuous columns:', continuous_cols)
print('Categorical columns:', categorical_cols)

# Display countplot for each categorical column in DataFrame df_t
for column in categorical_cols:
    plt.figure(figsize=(16, 6))
    sns.countplot(x=column, data=df_t)
    plt.title(column)
    plt.show()

# All categorical features are unbalanced.

# Display barplot of average 'track_popularity' for each unique value in each categorical column in DataFrame df_t
for column in categorical_cols:
    dataset = df_t.copy()
    plt.figure(figsize=(16, 6))
    sns.barplot(x=column, y=dataset['track_popularity'], data=dataset, estimator=np.mean)
    plt.show()

# Average popularity scores for genres
top_five_genres = df_t.groupby("playlist_genre")["track_popularity"].mean().nlargest(5)
table = pd.DataFrame({'playlist_genre': top_five_genres.index, 'mean_popularity': top_five_genres.values}).set_index('playlist_genre')

# Display the results
print(table)

# Genre pop outperforms Latin genre in average popularity score.
df_t = df_t.drop(['playlist_genre'], axis=1)

# Display information about DataFrame df_t
df_t.info()

# Calculate number of rows, columns, and total samples in DataFrame df_t
num_rows = df_t.shape[0]
num_col = df_t.shape[1]
total_samples = num_rows * num_col

# Display the results
print('Number of rows:', num_rows)
print('Number of columns:', num_col)
print('Total number of samples:', total_samples)

### Modeling

# First, determine K for KMeans clustering.
# Followed by running KMeans clustering model using TensorFlow and Pycaret to determine patterns and/or relationships in the data.

# Select specific numeric columns and scale them
num_cols = ["duration_ms", "tempo", "loudness", "speechiness", "acousticness", "instrumentalness", "
